{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hew/python/genhance/ACE2\n",
      "'Analyze Controlled Generators.ipynb'\r\n",
      "'Analyze FoldX Results.ipynb'\r\n",
      " \u001B[0m\u001B[01;34mbackup\u001B[0m/\r\n",
      " generate_sequences_congen.py\r\n",
      " \u001B[01;34mgeneration_scripts\u001B[0m/\r\n",
      "'Prepare Sequences for FoldX Cooptim.ipynb'\r\n",
      "'Prepare Sequences for FoldX.ipynb'\r\n",
      " train_controlled_generator_continue.py\r\n",
      " train_controlled_generator_cooptim.py\r\n",
      " train_controlled_generator.py\r\n",
      " \u001B[01;34mtraining_scripts\u001B[0m/\r\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%cd '/home/hew/python/genhance/ACE2'\n",
    "%ls"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import shutil\n",
    "import scipy\n",
    "import typing\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5Tokenizer\n",
    "from transformers.optimization import AdamW, get_scheduler\n",
    "from transformers_custom import MT5ForConditionalGenerationWithLatentSpace, MT5Config\n",
    "from tensorboardX import SummaryWriter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Debug"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "pretrained_dir = 'Rostlab/prot_t5_xl_uniref50'\n",
    "cache_dir = '/home/hew/storage/storage/genhance/pretrained/'\n",
    "src_json = '/home/hew/python/genhance/temp/config.json'\n",
    "clone_dir = '/home/hew/storage/storage/genhance/pretrained/prot_t5_xl_uniref50'\n",
    "# pretrained_dir = '/home/hew/storage/storage/genhance/ckpts/congen1/results/step_35000'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading spiece.model:   0%|          | 0.00/238k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0c5dfdf24fa24fd4ace9cf2491597990"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "820f7767a43e4638979d5b31a1902549"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e73417d6715e451f82224b4d36defd9a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/546 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e348939bd913416e8065b27db56c378e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cuda:0\n"
     ]
    }
   ],
   "source": [
    "print('Loading model')\n",
    "tokenizer = T5Tokenizer.from_pretrained(pretrained_dir)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device', device)\n",
    "tokenizer.add_special_tokens({\"cls_token\": \"<cls>\"})\n",
    "assert tokenizer.cls_token == \"<cls>\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/546 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca8416772687465f85d9fd3b3c279c3c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading:   0%|          | 0.00/11.3G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "926bf41455e8464ea4b397166d08b9c9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== T5 Model T5ForConditionalGenerationWithLatentSpace Initialization start ====================\n",
      "========== Initialize T5ForConditionalGenerationWithLatentSpace ==========\n",
      "latent_space_type:  wae\n",
      "wae_z_enc_type:  deterministic\n",
      "separate_latent_enc:  False\n",
      "separate_latent_dec:  False\n",
      "mmd_method:  rf\n",
      "sigma_mmd:  None\n",
      "rf_dim_mmd:  None\n",
      "dim_target_kl:  0.5\n",
      "latent_size:  1024\n",
      "latent_pooler:  cls\n",
      "pool_enc_hidden_states_for_dec:  True\n",
      "mask_non_target_z_vector:  False\n",
      "separate_targetattr_head:  False\n",
      "do_mi:  False\n",
      "========== Initialize T5ForConditionalGenerationWithLatentSpace ==========\n",
      "==================== T5 Model T5ForConditionalGenerationWithLatentSpace Initialization end ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MT5ForConditionalGenerationWithLatentSpace were not initialized from the model checkpoint at Rostlab/prot_t5_xl_uniref50 and are newly initialized: ['vae_enc.weight', 'vae_dec.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 17s, sys: 29.9 s, total: 3min 47s\n",
      "Wall time: 3min 48s\n"
     ]
    },
    {
     "data": {
      "text/plain": "Embedding(129, 1024)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = MT5ForConditionalGenerationWithLatentSpace.from_pretrained(pretrained_dir)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "MT5ForConditionalGenerationWithLatentSpace(\n  (shared): Embedding(129, 1024)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(129, 1024)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n              (relative_attention_bias): Embedding(32, 32)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (2): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (3): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (4): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (5): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (6): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (7): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (8): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (9): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (10): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (11): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (12): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (13): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (14): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (15): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (16): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (17): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (18): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (19): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (20): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (21): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (22): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (23): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (vae_enc): Linear(in_features=1024, out_features=1024, bias=False)\n  (vae_dec): Linear(in_features=1024, out_features=1024, bias=False)\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(129, 1024)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n              (relative_attention_bias): Embedding(32, 32)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (2): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (3): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (4): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (5): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (6): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (7): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (8): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (9): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (10): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (11): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (12): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (13): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (14): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (15): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (16): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (17): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (18): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (19): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (20): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (21): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (22): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (23): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=1024, out_features=129, bias=False)\n)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.01 s, sys: 11.2 s, total: 18.2 s\n",
      "Wall time: 18.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "'/home/hew/temp/config.json'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "temp_dir = '/home/hew/temp/'\n",
    "os.mkdir(temp_dir)\n",
    "saved_weights_file = temp_dir + 'pytorch_model.bin'\n",
    "torch.save(model.state_dict(), saved_weights_file)\n",
    "shutil.copy(src_json, temp_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== T5 Model T5ForConditionalGenerationWithLatentSpace Initialization start ====================\n",
      "========== Initialize T5ForConditionalGenerationWithLatentSpace ==========\n",
      "latent_space_type:  wae\n",
      "wae_z_enc_type:  deterministic\n",
      "separate_latent_enc:  False\n",
      "separate_latent_dec:  False\n",
      "mmd_method:  rf\n",
      "sigma_mmd:  None\n",
      "rf_dim_mmd:  None\n",
      "dim_target_kl:  0.5\n",
      "latent_size:  1024\n",
      "latent_pooler:  cls\n",
      "pool_enc_hidden_states_for_dec:  True\n",
      "mask_non_target_z_vector:  False\n",
      "separate_targetattr_head:  False\n",
      "do_mi:  False\n",
      "========== Initialize T5ForConditionalGenerationWithLatentSpace ==========\n",
      "==================== T5 Model T5ForConditionalGenerationWithLatentSpace Initialization end ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/hew/storage/storage/genhance/pretrained/ were not used when initializing MT5ForConditionalGenerationWithLatentSpace: ['encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.block.6.layer.0.layer_norm.weight', 'encoder.block.6.layer.1.DenseReluDense.wi.weight', 'encoder.block.6.layer.1.DenseReluDense.wo.weight', 'encoder.block.6.layer.1.layer_norm.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.layer_norm.weight', 'encoder.block.7.layer.1.DenseReluDense.wi.weight', 'encoder.block.7.layer.1.DenseReluDense.wo.weight', 'encoder.block.7.layer.1.layer_norm.weight', 'encoder.block.8.layer.0.SelfAttention.q.weight', 'encoder.block.8.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.block.8.layer.0.layer_norm.weight', 'encoder.block.8.layer.1.DenseReluDense.wi.weight', 'encoder.block.8.layer.1.DenseReluDense.wo.weight', 'encoder.block.8.layer.1.layer_norm.weight', 'encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.block.9.layer.0.SelfAttention.k.weight', 'encoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.0.layer_norm.weight', 'encoder.block.9.layer.1.DenseReluDense.wi.weight', 'encoder.block.9.layer.1.DenseReluDense.wo.weight', 'encoder.block.9.layer.1.layer_norm.weight', 'encoder.block.10.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.block.10.layer.0.SelfAttention.v.weight', 'encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.layer_norm.weight', 'encoder.block.10.layer.1.DenseReluDense.wi.weight', 'encoder.block.10.layer.1.DenseReluDense.wo.weight', 'encoder.block.10.layer.1.layer_norm.weight', 'encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.block.11.layer.0.SelfAttention.v.weight', 'encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.block.11.layer.0.layer_norm.weight', 'encoder.block.11.layer.1.DenseReluDense.wi.weight', 'encoder.block.11.layer.1.DenseReluDense.wo.weight', 'encoder.block.11.layer.1.layer_norm.weight', 'encoder.block.12.layer.0.SelfAttention.q.weight', 'encoder.block.12.layer.0.SelfAttention.k.weight', 'encoder.block.12.layer.0.SelfAttention.v.weight', 'encoder.block.12.layer.0.SelfAttention.o.weight', 'encoder.block.12.layer.0.layer_norm.weight', 'encoder.block.12.layer.1.DenseReluDense.wi.weight', 'encoder.block.12.layer.1.DenseReluDense.wo.weight', 'encoder.block.12.layer.1.layer_norm.weight', 'encoder.block.13.layer.0.SelfAttention.q.weight', 'encoder.block.13.layer.0.SelfAttention.k.weight', 'encoder.block.13.layer.0.SelfAttention.v.weight', 'encoder.block.13.layer.0.SelfAttention.o.weight', 'encoder.block.13.layer.0.layer_norm.weight', 'encoder.block.13.layer.1.DenseReluDense.wi.weight', 'encoder.block.13.layer.1.DenseReluDense.wo.weight', 'encoder.block.13.layer.1.layer_norm.weight', 'encoder.block.14.layer.0.SelfAttention.q.weight', 'encoder.block.14.layer.0.SelfAttention.k.weight', 'encoder.block.14.layer.0.SelfAttention.v.weight', 'encoder.block.14.layer.0.SelfAttention.o.weight', 'encoder.block.14.layer.0.layer_norm.weight', 'encoder.block.14.layer.1.DenseReluDense.wi.weight', 'encoder.block.14.layer.1.DenseReluDense.wo.weight', 'encoder.block.14.layer.1.layer_norm.weight', 'encoder.block.15.layer.0.SelfAttention.q.weight', 'encoder.block.15.layer.0.SelfAttention.k.weight', 'encoder.block.15.layer.0.SelfAttention.v.weight', 'encoder.block.15.layer.0.SelfAttention.o.weight', 'encoder.block.15.layer.0.layer_norm.weight', 'encoder.block.15.layer.1.DenseReluDense.wi.weight', 'encoder.block.15.layer.1.DenseReluDense.wo.weight', 'encoder.block.15.layer.1.layer_norm.weight', 'encoder.block.16.layer.0.SelfAttention.q.weight', 'encoder.block.16.layer.0.SelfAttention.k.weight', 'encoder.block.16.layer.0.SelfAttention.v.weight', 'encoder.block.16.layer.0.SelfAttention.o.weight', 'encoder.block.16.layer.0.layer_norm.weight', 'encoder.block.16.layer.1.DenseReluDense.wi.weight', 'encoder.block.16.layer.1.DenseReluDense.wo.weight', 'encoder.block.16.layer.1.layer_norm.weight', 'encoder.block.17.layer.0.SelfAttention.q.weight', 'encoder.block.17.layer.0.SelfAttention.k.weight', 'encoder.block.17.layer.0.SelfAttention.v.weight', 'encoder.block.17.layer.0.SelfAttention.o.weight', 'encoder.block.17.layer.0.layer_norm.weight', 'encoder.block.17.layer.1.DenseReluDense.wi.weight', 'encoder.block.17.layer.1.DenseReluDense.wo.weight', 'encoder.block.17.layer.1.layer_norm.weight', 'encoder.block.18.layer.0.SelfAttention.q.weight', 'encoder.block.18.layer.0.SelfAttention.k.weight', 'encoder.block.18.layer.0.SelfAttention.v.weight', 'encoder.block.18.layer.0.SelfAttention.o.weight', 'encoder.block.18.layer.0.layer_norm.weight', 'encoder.block.18.layer.1.DenseReluDense.wi.weight', 'encoder.block.18.layer.1.DenseReluDense.wo.weight', 'encoder.block.18.layer.1.layer_norm.weight', 'encoder.block.19.layer.0.SelfAttention.q.weight', 'encoder.block.19.layer.0.SelfAttention.k.weight', 'encoder.block.19.layer.0.SelfAttention.v.weight', 'encoder.block.19.layer.0.SelfAttention.o.weight', 'encoder.block.19.layer.0.layer_norm.weight', 'encoder.block.19.layer.1.DenseReluDense.wi.weight', 'encoder.block.19.layer.1.DenseReluDense.wo.weight', 'encoder.block.19.layer.1.layer_norm.weight', 'encoder.block.20.layer.0.SelfAttention.q.weight', 'encoder.block.20.layer.0.SelfAttention.k.weight', 'encoder.block.20.layer.0.SelfAttention.v.weight', 'encoder.block.20.layer.0.SelfAttention.o.weight', 'encoder.block.20.layer.0.layer_norm.weight', 'encoder.block.20.layer.1.DenseReluDense.wi.weight', 'encoder.block.20.layer.1.DenseReluDense.wo.weight', 'encoder.block.20.layer.1.layer_norm.weight', 'encoder.block.21.layer.0.SelfAttention.q.weight', 'encoder.block.21.layer.0.SelfAttention.k.weight', 'encoder.block.21.layer.0.SelfAttention.v.weight', 'encoder.block.21.layer.0.SelfAttention.o.weight', 'encoder.block.21.layer.0.layer_norm.weight', 'encoder.block.21.layer.1.DenseReluDense.wi.weight', 'encoder.block.21.layer.1.DenseReluDense.wo.weight', 'encoder.block.21.layer.1.layer_norm.weight', 'encoder.block.22.layer.0.SelfAttention.q.weight', 'encoder.block.22.layer.0.SelfAttention.k.weight', 'encoder.block.22.layer.0.SelfAttention.v.weight', 'encoder.block.22.layer.0.SelfAttention.o.weight', 'encoder.block.22.layer.0.layer_norm.weight', 'encoder.block.22.layer.1.DenseReluDense.wi.weight', 'encoder.block.22.layer.1.DenseReluDense.wo.weight', 'encoder.block.22.layer.1.layer_norm.weight', 'encoder.block.23.layer.0.SelfAttention.q.weight', 'encoder.block.23.layer.0.SelfAttention.k.weight', 'encoder.block.23.layer.0.SelfAttention.v.weight', 'encoder.block.23.layer.0.SelfAttention.o.weight', 'encoder.block.23.layer.0.layer_norm.weight', 'encoder.block.23.layer.1.DenseReluDense.wi.weight', 'encoder.block.23.layer.1.DenseReluDense.wo.weight', 'encoder.block.23.layer.1.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.2.layer_norm.weight']\n",
      "- This IS expected if you are initializing MT5ForConditionalGenerationWithLatentSpace from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MT5ForConditionalGenerationWithLatentSpace from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.7 s, sys: 11 s, total: 32.7 s\n",
      "Wall time: 25.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model_2 = MT5ForConditionalGenerationWithLatentSpace.from_pretrained(cache_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "MT5ForConditionalGenerationWithLatentSpace(\n  (shared): Embedding(129, 1024)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(129, 1024)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n              (relative_attention_bias): Embedding(32, 32)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (2): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (3): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (4): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (5): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (vae_enc): Linear(in_features=1024, out_features=1024, bias=False)\n  (vae_dec): Linear(in_features=1024, out_features=1024, bias=False)\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(129, 1024)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n              (relative_attention_bias): Embedding(32, 32)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (2): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (3): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (4): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (5): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=1024, out_features=4096, bias=False)\n              (k): Linear(in_features=1024, out_features=4096, bias=False)\n              (v): Linear(in_features=1024, out_features=4096, bias=False)\n              (o): Linear(in_features=4096, out_features=1024, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseReluDense(\n              (wi): Linear(in_features=1024, out_features=16384, bias=False)\n              (wo): Linear(in_features=16384, out_features=1024, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=1024, out_features=129, bias=False)\n)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "cache_config = '/home/hew/storage/storage/genhance/pretrained/config.json'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "MT5Config {\n  \"architectures\": [\n    \"T5ForConditionalGeneration\"\n  ],\n  \"d_ff\": 16384,\n  \"d_kv\": 128,\n  \"d_model\": 1024,\n  \"decoder_start_token_id\": 0,\n  \"dropout_rate\": 0.1,\n  \"eos_token_id\": 1,\n  \"feed_forward_proj\": \"relu\",\n  \"initializer_factor\": 1.0,\n  \"is_encoder_decoder\": true,\n  \"layer_norm_epsilon\": 1e-06,\n  \"model_type\": \"mt5\",\n  \"n_positions\": 512,\n  \"num_decoder_layers\": 6,\n  \"num_heads\": 32,\n  \"num_layers\": 6,\n  \"output_past\": true,\n  \"pad_token_id\": 0,\n  \"relative_attention_num_buckets\": 32,\n  \"tie_word_embeddings\": false,\n  \"tokenizer_class\": \"T5Tokenizer\",\n  \"transformers_version\": \"4.4.0.dev0\",\n  \"use_cache\": true,\n  \"vocab_size\": 129\n}"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5config = MT5Config.from_pretrained(cache_config)\n",
    "t5config.num_layers = 6\n",
    "t5config.num_decoder_layers = 6\n",
    "t5config"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "seed = 42\n",
    "data_dir = '/home/hew/storage/storage/genhance/data/'\n",
    "pretrained_dir = '/home/hew/storage/storage/genhance/pretrained/'\n",
    "cache_dir = '/home/hew/storage/storage/genhance/pretrained/'\n",
    "src_json = '/home/hew/python/genhance/temp/config.json'\n",
    "output_dir = '/home/hew/storage/storage/genhance/ckpts/congen2/results/'\n",
    "logging_dir = '/home/hew/python/genhance/tensorboard/congen2'\n",
    "train_split_name = \"train\"\n",
    "eval_split_name = \"valid\"\n",
    "per_device_train_batch_size = 32\n",
    "per_device_eval_batch_size = 128\n",
    "\n",
    "save_total_limit = 2\n",
    "lr = 1e-04\n",
    "num_train_epochs = 30\n",
    "train_ratio = 0.9\n",
    "lambda_contrastive = 1.0\n",
    "latent_pooler = 'cls'\n",
    "pool_enc_hidden_states_for_dec = True\n",
    "latent_space_type = 'wae'\n",
    "beta_start_step = 200000\n",
    "latent_size = 1024\n",
    "wae_z_enc_type = 'deterministic'\n",
    "no_separate_latent_enc = True\n",
    "no_separate_latent_dec = True\n",
    "lambda_contrastive_cyc = 1.0\n",
    "contrastive_cyc_start_step = 200000\n",
    "eval_steps = 250\n",
    "save_steps = 1000\n",
    "logging_steps = 50\n",
    "\n",
    "mask_non_target_z_vector = False\n",
    "separate_targetattr_head = False\n",
    "z_tar_vector_dim = 1\n",
    "do_mi = False\n",
    "separate_latent_enc = False\n",
    "separate_latent_dec = False\n",
    "dim_target_kl = 0.5\n",
    "mmd_method = 'rf'\n",
    "sigma_mmd = None\n",
    "rf_dim_mmd = None\n",
    "\n",
    "warmup_steps = 500\n",
    "num_warmup_steps = 0\n",
    "weight_decay = 0.01\n",
    "beta = 1.0\n",
    "beta_ratio_increase = 0.25\n",
    "beta_ratio_zero = 0.25\n",
    "use_beta_schedule = False\n",
    "lambda_logvar_L1 = 0.0\n",
    "lambda_logvar_KL = 0.0\n",
    "# lambda_contrastive_perturb_cyc = 0.0\n",
    "# contrastive_perturb_cyc_start_step = -1\n",
    "lambda_contrastive_perturb_cyc = 1.0\n",
    "contrastive_perturb_cyc_start_step = 1\n",
    "pc_perturb_type = 'std'\n",
    "pc_perturb = -0.25\n",
    "\n",
    "property = 'ddG'\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class PKLDFDatasetForGen(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_file: typing.Union[str, Path],\n",
    "            in_memory: bool = False,\n",
    "            split: str = 'train',\n",
    "            train_ratio: float = 1,\n",
    "            data_subset='full'\n",
    "            ):\n",
    "\n",
    "        data_file = Path(data_file)\n",
    "        if not data_file.exists():\n",
    "            raise FileNotFoundError(data_file)\n",
    "\n",
    "        df = pd.read_pickle(data_file)\n",
    "\n",
    "        if train_ratio != 1:\n",
    "            shuffled_df = df.sort_index()\n",
    "            train_num_samples = int(len(shuffled_df)*train_ratio)\n",
    "            if split == 'train':\n",
    "                final_df = shuffled_df.iloc[:train_num_samples]\n",
    "            elif split == 'valid':\n",
    "                final_df = shuffled_df.iloc[train_num_samples:]\n",
    "            else:\n",
    "                final_df = df\n",
    "        else:\n",
    "            final_df = df\n",
    "\n",
    "        # split into subset if not full training set\n",
    "        if data_subset != 'full':\n",
    "            ddG_sorted_final_df = final_df.sort_values(by='ddG', ascending=True)\n",
    "            train_subset_num_samples = int(data_subset*len(ddG_sorted_final_df))\n",
    "            final_df = ddG_sorted_final_df.iloc[:train_subset_num_samples]\n",
    "\n",
    "        print(\"split: \", split)\n",
    "        print(\"data_file: \", data_file)\n",
    "        print(\"len(final_df): \", len(final_df))\n",
    "\n",
    "        self.df = final_df\n",
    "        num_examples = len(final_df)\n",
    "        self._num_examples = num_examples\n",
    "\n",
    "        if in_memory:\n",
    "            cache = [None]*num_examples\n",
    "            self._cache = cache\n",
    "\n",
    "        self._in_memory = in_memory\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self._num_examples\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        if not 0 <= index < self._num_examples:\n",
    "            raise IndexError(index)\n",
    "\n",
    "        if self._in_memory and self._cache[index] is not None:\n",
    "            item = self._cache[index]\n",
    "        else:\n",
    "            row = self.df.iloc[index]\n",
    "            item = {}\n",
    "            item['ddG'] = row['ddG']\n",
    "            item['solubility'] = row['solubility']\n",
    "            item['input_ids'] = row['MT_seq']\n",
    "            item['labels'] = row['MT_seq']\n",
    "            item['id'] = str(index)\n",
    "            if self._in_memory:\n",
    "                self._cache[index] = item\n",
    "\n",
    "        return item\n",
    "\n",
    "\n",
    "class CustomStabilityDatasetForGenLatentSpace(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_path: typing.Union[str, Path],\n",
    "            split: str,\n",
    "            tokenizer,\n",
    "            in_memory: bool = False,\n",
    "            train_ratio: float = 1,\n",
    "            ):\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        if split == 'valid':\n",
    "            file_prefix = 'train'\n",
    "        else:\n",
    "            file_prefix = split\n",
    "\n",
    "        data_path = Path(data_path)\n",
    "        data_file = f'{file_prefix}_tophalf_ddG_solubility.pkl'  # tophalf\n",
    "        self.data = PKLDFDatasetForGen(data_path/data_file, in_memory, split, train_ratio, data_subset='full')\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        item = self.data[index]\n",
    "        input_ids = item['input_ids']\n",
    "        labels = item['labels']\n",
    "        if property == 'ddG':\n",
    "            ddG = item['ddG']\n",
    "            solubility = None\n",
    "        elif property == 'solubility':\n",
    "            ddG = None\n",
    "            solubility = item['solubility']\n",
    "        elif property == 'ddG_solubility':\n",
    "            ddG = item['ddG']\n",
    "            solubility = item['solubility']\n",
    "        return input_ids, labels, ddG, solubility\n",
    "\n",
    "    def collate_fn(self, batch: typing.List[typing.Tuple[typing.Any, ...]]) -> typing.Dict[str, torch.Tensor]:\n",
    "        input_ids, labels, ddG, solubility = tuple(zip(*batch))\n",
    "        prefix = \"<cls> \"\n",
    "        # length = <cls> + 83 + </s> = 85\n",
    "        input_ids = [prefix + \" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in input_ids]\n",
    "        labels = [prefix + \" \".join(list(re.sub(r\"[UZOB]\", \"X\", sequence))) for sequence in labels]\n",
    "        input_ids = self.tokenizer.batch_encode_plus(input_ids, add_special_tokens=True, padding=\"longest\", return_tensors='pt')[\n",
    "            'input_ids']\n",
    "        labels = self.tokenizer.batch_encode_plus(labels, add_special_tokens=True, padding=\"longest\", return_tensors='pt')['input_ids']\n",
    "        ddG = torch.Tensor(ddG) if None not in ddG else None\n",
    "        solubility = torch.Tensor(solubility) if None not in solubility else None\n",
    "        return {\n",
    "            'input_ids' : input_ids,\n",
    "            'labels'    : labels,\n",
    "            'ddG'       : ddG,\n",
    "            'solubility': solubility\n",
    "            }\n",
    "\n",
    "\n",
    "def spearmanr(target, prediction):\n",
    "    target_array = np.asarray(target)\n",
    "    prediction_array = np.asarray(prediction)\n",
    "    return scipy.stats.spearmanr(target_array, prediction_array).correlation\n",
    "\n",
    "\n",
    "def evaluate(model, eval_loader, do_mi=False, do_spearmanr=True, latent_space_type='wae', return_pred=False):\n",
    "    eval_contrastive_loss_total = 0\n",
    "    eval_lm_loss_total = 0\n",
    "    if do_mi:\n",
    "        eval_mi_head_loss_total = 0\n",
    "    if latent_space_type in ['vae', 'wae']:\n",
    "        eval_z_regu_loss_total = 0\n",
    "    model.eval()\n",
    "    num_eval_batch = 0\n",
    "\n",
    "    ddG_preds = []\n",
    "    ddG_targs = []\n",
    "    solubility_preds = []\n",
    "    solubility_targs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(tqdm(eval_loader)):\n",
    "            input_ids = batch['input_ids'].to(model.device)\n",
    "            labels = batch['labels'].to(model.device)\n",
    "            ddG_targets = batch['ddG'].to(model.device) if 'ddG' in batch else None\n",
    "            solubility_targets = batch['solubility'].to(model.device) if 'solubility' in batch else None\n",
    "            contrast_targets = [ddG_targets, solubility_targets]\n",
    "\n",
    "            if do_mi:\n",
    "                model_outputs = model(input_ids, labels=labels, contrast_targets=contrast_targets)\n",
    "                outputs, contrastive_loss, contrastive_value, mi_head_loss = model_outputs[0], model_outputs[1], model_outputs[2], \\\n",
    "                    model_outputs[3]\n",
    "                eval_mi_head_loss_total = eval_mi_head_loss_total + mi_head_loss\n",
    "            else:\n",
    "                model_outputs = model(input_ids, labels=labels, contrast_targets=contrast_targets)\n",
    "                outputs, contrastive_loss, contrastive_value = model_outputs[0], model_outputs[1], model_outputs[2]\n",
    "\n",
    "            if latent_space_type in ['vae', 'wae']:\n",
    "                z_regu_output = model_outputs[-1]\n",
    "                if type(z_regu_output) is dict:\n",
    "                    z_regu_loss = z_regu_output['z_regu_loss']\n",
    "                else:\n",
    "                    z_regu_loss = z_regu_output\n",
    "\n",
    "            # contrastive_value: [batch_size, 1] or [batch_size, 2], ddG_targets: [batch_size]\n",
    "            if property == 'ddG':\n",
    "                for pred, target in zip(contrastive_value.squeeze().cpu().numpy(), ddG_targets.cpu().numpy()):\n",
    "                    ddG_targs.append(target)\n",
    "                    ddG_preds.append(pred)\n",
    "            elif property == 'solubility':\n",
    "                for pred, target in zip(contrastive_value.squeeze().cpu().numpy(), ddG_targets.cpu().numpy()):\n",
    "                    solubility_targs.append(target)\n",
    "                    solubility_preds.append(pred)\n",
    "            elif property == 'ddG_solubility':\n",
    "                for pred, target in zip(contrastive_value[:, 0].squeeze().cpu().numpy(), ddG_targets.cpu().numpy()):\n",
    "                    ddG_targs.append(target)\n",
    "                    ddG_preds.append(pred)\n",
    "                for pred, target in zip(contrastive_value[:, 1].squeeze().cpu().numpy(), ddG_targets.cpu().numpy()):\n",
    "                    solubility_targs.append(target)\n",
    "                    solubility_preds.append(pred)\n",
    "\n",
    "            lm_loss = outputs.loss\n",
    "\n",
    "            eval_contrastive_loss_total = eval_contrastive_loss_total + contrastive_loss\n",
    "            eval_lm_loss_total = eval_lm_loss_total + lm_loss\n",
    "\n",
    "            if latent_space_type in ['vae', 'wae']:\n",
    "                eval_z_regu_loss_total = eval_z_regu_loss_total + z_regu_loss\n",
    "\n",
    "            num_eval_batch += 1\n",
    "\n",
    "    eval_lm_loss = eval_lm_loss_total/num_eval_batch\n",
    "    eval_contrastive_loss = eval_contrastive_loss_total/num_eval_batch\n",
    "    eval_output = {\n",
    "        \"lm_loss\"         : eval_lm_loss,\n",
    "        \"contrastive_loss\": eval_contrastive_loss,\n",
    "        }\n",
    "\n",
    "    if do_mi:\n",
    "        eval_mi_head_loss_total = eval_mi_head_loss_total/num_eval_batch\n",
    "        eval_output['mi_head_loss'] = eval_mi_head_loss_total\n",
    "\n",
    "    if latent_space_type in ['vae', 'wae']:\n",
    "        eval_z_regu_loss_total = eval_z_regu_loss_total/num_eval_batch\n",
    "        eval_output['z_regu_loss'] = eval_z_regu_loss_total\n",
    "\n",
    "    if do_spearmanr:\n",
    "        ddG_spearman = 0\n",
    "        solubility_spearman = 0\n",
    "        if len(ddG_targs) > 0 and len(ddG_preds) > 0:\n",
    "            ddG_spearman = spearmanr(ddG_targs, ddG_preds)\n",
    "        if len(solubility_targs) > 0 and len(solubility_preds) > 0:\n",
    "            solubility_spearman = spearmanr(solubility_targs, solubility_preds)\n",
    "\n",
    "        print()\n",
    "        print(\"========== evaluation finished ==========\")\n",
    "        print(\"ddG_spearman: \", ddG_spearman)\n",
    "        print(\"solubility_spearman: \", solubility_spearman)\n",
    "        print(\"========== evaluation finished ==========\")\n",
    "        eval_output['ddG_spearman'] = ddG_spearman\n",
    "        eval_output['solubility_spearman'] = solubility_spearman\n",
    "\n",
    "    if return_pred:\n",
    "        eval_output['ddG_preds'] = ddG_preds\n",
    "        eval_output['ddG_targs'] = ddG_targs\n",
    "        eval_output['solubility_preds'] = solubility_preds\n",
    "        eval_output['solubility_targs'] = solubility_targs\n",
    "\n",
    "    return eval_output\n",
    "\n",
    "\n",
    "def frange_cycle_zero_linear(n_iter, start=0.0, stop=1.0, n_cycle=4, ratio_increase=0.5, ratio_zero=0.3):\n",
    "    L = np.ones(n_iter)*stop\n",
    "    period = n_iter/n_cycle\n",
    "    step = (stop - start)/(period*ratio_increase)  # linear schedule\n",
    "\n",
    "    for c in range(n_cycle):\n",
    "        v, i = start, 0\n",
    "        while v <= stop and (int(i + c*period) < n_iter):\n",
    "            if i < period*ratio_zero:\n",
    "                L[int(i + c*period)] = start\n",
    "            else:\n",
    "                L[int(i + c*period)] = v\n",
    "                v += step\n",
    "            i += 1\n",
    "    return L\n",
    "\n",
    "\n",
    "def mkdir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        print(\"mkdir: \", path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "latent_space_args:  {'latent_pooler': 'cls', 'pool_enc_hidden_states_for_dec': True, 'mask_non_target_z_vector': False, 'separate_targetattr_head': False, 'z_tar_vector_dim': 1, 'do_mi': False, 'latent_space_type': 'wae', 'separate_latent_enc': False, 'separate_latent_dec': False, 'wae_z_enc_type': 'deterministic', 'latent_size': 1024, 'dim_target_kl': 0.5, 'mmd_method': 'rf', 'sigma_mmd': None, 'rf_dim_mmd': None}\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "output_dir = Path(output_dir)\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "latent_space_type = latent_space_type\n",
    "wae_z_enc_type = wae_z_enc_type\n",
    "latent_space_args = {\n",
    "    'latent_pooler'                 : latent_pooler,\n",
    "    'pool_enc_hidden_states_for_dec': pool_enc_hidden_states_for_dec,\n",
    "    'mask_non_target_z_vector'      : mask_non_target_z_vector,\n",
    "    'separate_targetattr_head'      : separate_targetattr_head,\n",
    "    'z_tar_vector_dim'              : z_tar_vector_dim,\n",
    "    'do_mi'                         : do_mi,\n",
    "    'latent_space_type'             : latent_space_type,\n",
    "    'separate_latent_enc'           : separate_latent_enc,\n",
    "    'separate_latent_dec'           : separate_latent_dec,\n",
    "    'wae_z_enc_type'                : wae_z_enc_type,\n",
    "    'latent_size'                   : latent_size,\n",
    "    'dim_target_kl'                 : dim_target_kl,\n",
    "    'mmd_method'                    : mmd_method,\n",
    "    'sigma_mmd'                     : sigma_mmd,\n",
    "    'rf_dim_mmd'                    : rf_dim_mmd,\n",
    "    }\n",
    "\n",
    "print('-'*100)\n",
    "print(\"latent_space_args: \", latent_space_args)\n",
    "print('-'*100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split:  train\n",
      "data_file:  /home/hew/storage/storage/genhance/data/train_tophalf_ddG_solubility.pkl\n",
      "len(final_df):  112500\n",
      "split:  valid\n",
      "data_file:  /home/hew/storage/storage/genhance/data/train_tophalf_ddG_solubility.pkl\n",
      "len(final_df):  12500\n",
      "----------------------------------------------------------------------------------------------------\n",
      "t5config MT5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 16384,\n",
      "  \"d_kv\": 128,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 32,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.4.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 129\n",
      "}\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_uniref50', cache_dir=cache_dir)\n",
    "tokenizer.add_special_tokens({\"cls_token\": \"<cls>\"})\n",
    "assert tokenizer.cls_token == \"<cls>\"\n",
    "\n",
    "train_dataset = CustomStabilityDatasetForGenLatentSpace(data_dir, train_split_name, train_ratio=train_ratio, tokenizer=tokenizer)\n",
    "eval_dataset = CustomStabilityDatasetForGenLatentSpace(data_dir, eval_split_name, train_ratio=train_ratio, tokenizer=tokenizer)\n",
    "num_training_steps = num_train_epochs*len(train_dataset)//per_device_train_batch_size\n",
    "\n",
    "# Train data set-up\n",
    "train_loader = DataLoader(train_dataset, batch_size=per_device_train_batch_size, shuffle=True,\n",
    "                          num_workers=0, collate_fn=train_dataset.collate_fn)\n",
    "\n",
    "# Eval data set-up\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=per_device_eval_batch_size, shuffle=False,\n",
    "                         num_workers=0, collate_fn=train_dataset.collate_fn)\n",
    "\n",
    "# load model (time consuming)\n",
    "device = torch.device('cuda:0')\n",
    "t5config = MT5Config.from_pretrained(pretrained_dir, cache_dir=cache_dir)\n",
    "print('-'*100)\n",
    "print('t5config', t5config)\n",
    "print('-'*100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== T5 Model T5ForConditionalGenerationWithLatentSpace Initialization start ====================\n",
      "========== Initialize T5ForConditionalGenerationWithLatentSpace ==========\n",
      "latent_space_type:  wae\n",
      "wae_z_enc_type:  deterministic\n",
      "separate_latent_enc:  False\n",
      "separate_latent_dec:  False\n",
      "mmd_method:  rf\n",
      "sigma_mmd:  None\n",
      "rf_dim_mmd:  None\n",
      "dim_target_kl:  0.5\n",
      "latent_size:  1024\n",
      "latent_pooler:  cls\n",
      "pool_enc_hidden_states_for_dec:  True\n",
      "mask_non_target_z_vector:  False\n",
      "separate_targetattr_head:  False\n",
      "do_mi:  False\n",
      "========== Initialize T5ForConditionalGenerationWithLatentSpace ==========\n",
      "==================== T5 Model T5ForConditionalGenerationWithLatentSpace Initialization end ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/hew/storage/storage/genhance/pretrained/ were not used when initializing MT5ForConditionalGenerationWithLatentSpace: ['encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.block.6.layer.0.layer_norm.weight', 'encoder.block.6.layer.1.DenseReluDense.wi.weight', 'encoder.block.6.layer.1.DenseReluDense.wo.weight', 'encoder.block.6.layer.1.layer_norm.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.layer_norm.weight', 'encoder.block.7.layer.1.DenseReluDense.wi.weight', 'encoder.block.7.layer.1.DenseReluDense.wo.weight', 'encoder.block.7.layer.1.layer_norm.weight', 'encoder.block.8.layer.0.SelfAttention.q.weight', 'encoder.block.8.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.block.8.layer.0.layer_norm.weight', 'encoder.block.8.layer.1.DenseReluDense.wi.weight', 'encoder.block.8.layer.1.DenseReluDense.wo.weight', 'encoder.block.8.layer.1.layer_norm.weight', 'encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.block.9.layer.0.SelfAttention.k.weight', 'encoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.0.layer_norm.weight', 'encoder.block.9.layer.1.DenseReluDense.wi.weight', 'encoder.block.9.layer.1.DenseReluDense.wo.weight', 'encoder.block.9.layer.1.layer_norm.weight', 'encoder.block.10.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.block.10.layer.0.SelfAttention.v.weight', 'encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.layer_norm.weight', 'encoder.block.10.layer.1.DenseReluDense.wi.weight', 'encoder.block.10.layer.1.DenseReluDense.wo.weight', 'encoder.block.10.layer.1.layer_norm.weight', 'encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.block.11.layer.0.SelfAttention.v.weight', 'encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.block.11.layer.0.layer_norm.weight', 'encoder.block.11.layer.1.DenseReluDense.wi.weight', 'encoder.block.11.layer.1.DenseReluDense.wo.weight', 'encoder.block.11.layer.1.layer_norm.weight', 'encoder.block.12.layer.0.SelfAttention.q.weight', 'encoder.block.12.layer.0.SelfAttention.k.weight', 'encoder.block.12.layer.0.SelfAttention.v.weight', 'encoder.block.12.layer.0.SelfAttention.o.weight', 'encoder.block.12.layer.0.layer_norm.weight', 'encoder.block.12.layer.1.DenseReluDense.wi.weight', 'encoder.block.12.layer.1.DenseReluDense.wo.weight', 'encoder.block.12.layer.1.layer_norm.weight', 'encoder.block.13.layer.0.SelfAttention.q.weight', 'encoder.block.13.layer.0.SelfAttention.k.weight', 'encoder.block.13.layer.0.SelfAttention.v.weight', 'encoder.block.13.layer.0.SelfAttention.o.weight', 'encoder.block.13.layer.0.layer_norm.weight', 'encoder.block.13.layer.1.DenseReluDense.wi.weight', 'encoder.block.13.layer.1.DenseReluDense.wo.weight', 'encoder.block.13.layer.1.layer_norm.weight', 'encoder.block.14.layer.0.SelfAttention.q.weight', 'encoder.block.14.layer.0.SelfAttention.k.weight', 'encoder.block.14.layer.0.SelfAttention.v.weight', 'encoder.block.14.layer.0.SelfAttention.o.weight', 'encoder.block.14.layer.0.layer_norm.weight', 'encoder.block.14.layer.1.DenseReluDense.wi.weight', 'encoder.block.14.layer.1.DenseReluDense.wo.weight', 'encoder.block.14.layer.1.layer_norm.weight', 'encoder.block.15.layer.0.SelfAttention.q.weight', 'encoder.block.15.layer.0.SelfAttention.k.weight', 'encoder.block.15.layer.0.SelfAttention.v.weight', 'encoder.block.15.layer.0.SelfAttention.o.weight', 'encoder.block.15.layer.0.layer_norm.weight', 'encoder.block.15.layer.1.DenseReluDense.wi.weight', 'encoder.block.15.layer.1.DenseReluDense.wo.weight', 'encoder.block.15.layer.1.layer_norm.weight', 'encoder.block.16.layer.0.SelfAttention.q.weight', 'encoder.block.16.layer.0.SelfAttention.k.weight', 'encoder.block.16.layer.0.SelfAttention.v.weight', 'encoder.block.16.layer.0.SelfAttention.o.weight', 'encoder.block.16.layer.0.layer_norm.weight', 'encoder.block.16.layer.1.DenseReluDense.wi.weight', 'encoder.block.16.layer.1.DenseReluDense.wo.weight', 'encoder.block.16.layer.1.layer_norm.weight', 'encoder.block.17.layer.0.SelfAttention.q.weight', 'encoder.block.17.layer.0.SelfAttention.k.weight', 'encoder.block.17.layer.0.SelfAttention.v.weight', 'encoder.block.17.layer.0.SelfAttention.o.weight', 'encoder.block.17.layer.0.layer_norm.weight', 'encoder.block.17.layer.1.DenseReluDense.wi.weight', 'encoder.block.17.layer.1.DenseReluDense.wo.weight', 'encoder.block.17.layer.1.layer_norm.weight', 'encoder.block.18.layer.0.SelfAttention.q.weight', 'encoder.block.18.layer.0.SelfAttention.k.weight', 'encoder.block.18.layer.0.SelfAttention.v.weight', 'encoder.block.18.layer.0.SelfAttention.o.weight', 'encoder.block.18.layer.0.layer_norm.weight', 'encoder.block.18.layer.1.DenseReluDense.wi.weight', 'encoder.block.18.layer.1.DenseReluDense.wo.weight', 'encoder.block.18.layer.1.layer_norm.weight', 'encoder.block.19.layer.0.SelfAttention.q.weight', 'encoder.block.19.layer.0.SelfAttention.k.weight', 'encoder.block.19.layer.0.SelfAttention.v.weight', 'encoder.block.19.layer.0.SelfAttention.o.weight', 'encoder.block.19.layer.0.layer_norm.weight', 'encoder.block.19.layer.1.DenseReluDense.wi.weight', 'encoder.block.19.layer.1.DenseReluDense.wo.weight', 'encoder.block.19.layer.1.layer_norm.weight', 'encoder.block.20.layer.0.SelfAttention.q.weight', 'encoder.block.20.layer.0.SelfAttention.k.weight', 'encoder.block.20.layer.0.SelfAttention.v.weight', 'encoder.block.20.layer.0.SelfAttention.o.weight', 'encoder.block.20.layer.0.layer_norm.weight', 'encoder.block.20.layer.1.DenseReluDense.wi.weight', 'encoder.block.20.layer.1.DenseReluDense.wo.weight', 'encoder.block.20.layer.1.layer_norm.weight', 'encoder.block.21.layer.0.SelfAttention.q.weight', 'encoder.block.21.layer.0.SelfAttention.k.weight', 'encoder.block.21.layer.0.SelfAttention.v.weight', 'encoder.block.21.layer.0.SelfAttention.o.weight', 'encoder.block.21.layer.0.layer_norm.weight', 'encoder.block.21.layer.1.DenseReluDense.wi.weight', 'encoder.block.21.layer.1.DenseReluDense.wo.weight', 'encoder.block.21.layer.1.layer_norm.weight', 'encoder.block.22.layer.0.SelfAttention.q.weight', 'encoder.block.22.layer.0.SelfAttention.k.weight', 'encoder.block.22.layer.0.SelfAttention.v.weight', 'encoder.block.22.layer.0.SelfAttention.o.weight', 'encoder.block.22.layer.0.layer_norm.weight', 'encoder.block.22.layer.1.DenseReluDense.wi.weight', 'encoder.block.22.layer.1.DenseReluDense.wo.weight', 'encoder.block.22.layer.1.layer_norm.weight', 'encoder.block.23.layer.0.SelfAttention.q.weight', 'encoder.block.23.layer.0.SelfAttention.k.weight', 'encoder.block.23.layer.0.SelfAttention.v.weight', 'encoder.block.23.layer.0.SelfAttention.o.weight', 'encoder.block.23.layer.0.layer_norm.weight', 'encoder.block.23.layer.1.DenseReluDense.wi.weight', 'encoder.block.23.layer.1.DenseReluDense.wo.weight', 'encoder.block.23.layer.1.layer_norm.weight', 'decoder.block.6.layer.0.SelfAttention.q.weight', 'decoder.block.6.layer.0.SelfAttention.k.weight', 'decoder.block.6.layer.0.SelfAttention.v.weight', 'decoder.block.6.layer.0.SelfAttention.o.weight', 'decoder.block.6.layer.0.layer_norm.weight', 'decoder.block.6.layer.1.EncDecAttention.q.weight', 'decoder.block.6.layer.1.EncDecAttention.k.weight', 'decoder.block.6.layer.1.EncDecAttention.v.weight', 'decoder.block.6.layer.1.EncDecAttention.o.weight', 'decoder.block.6.layer.1.layer_norm.weight', 'decoder.block.6.layer.2.DenseReluDense.wi.weight', 'decoder.block.6.layer.2.DenseReluDense.wo.weight', 'decoder.block.6.layer.2.layer_norm.weight', 'decoder.block.7.layer.0.SelfAttention.q.weight', 'decoder.block.7.layer.0.SelfAttention.k.weight', 'decoder.block.7.layer.0.SelfAttention.v.weight', 'decoder.block.7.layer.0.SelfAttention.o.weight', 'decoder.block.7.layer.0.layer_norm.weight', 'decoder.block.7.layer.1.EncDecAttention.q.weight', 'decoder.block.7.layer.1.EncDecAttention.k.weight', 'decoder.block.7.layer.1.EncDecAttention.v.weight', 'decoder.block.7.layer.1.EncDecAttention.o.weight', 'decoder.block.7.layer.1.layer_norm.weight', 'decoder.block.7.layer.2.DenseReluDense.wi.weight', 'decoder.block.7.layer.2.DenseReluDense.wo.weight', 'decoder.block.7.layer.2.layer_norm.weight', 'decoder.block.8.layer.0.SelfAttention.q.weight', 'decoder.block.8.layer.0.SelfAttention.k.weight', 'decoder.block.8.layer.0.SelfAttention.v.weight', 'decoder.block.8.layer.0.SelfAttention.o.weight', 'decoder.block.8.layer.0.layer_norm.weight', 'decoder.block.8.layer.1.EncDecAttention.q.weight', 'decoder.block.8.layer.1.EncDecAttention.k.weight', 'decoder.block.8.layer.1.EncDecAttention.v.weight', 'decoder.block.8.layer.1.EncDecAttention.o.weight', 'decoder.block.8.layer.1.layer_norm.weight', 'decoder.block.8.layer.2.DenseReluDense.wi.weight', 'decoder.block.8.layer.2.DenseReluDense.wo.weight', 'decoder.block.8.layer.2.layer_norm.weight', 'decoder.block.9.layer.0.SelfAttention.q.weight', 'decoder.block.9.layer.0.SelfAttention.k.weight', 'decoder.block.9.layer.0.SelfAttention.v.weight', 'decoder.block.9.layer.0.SelfAttention.o.weight', 'decoder.block.9.layer.0.layer_norm.weight', 'decoder.block.9.layer.1.EncDecAttention.q.weight', 'decoder.block.9.layer.1.EncDecAttention.k.weight', 'decoder.block.9.layer.1.EncDecAttention.v.weight', 'decoder.block.9.layer.1.EncDecAttention.o.weight', 'decoder.block.9.layer.1.layer_norm.weight', 'decoder.block.9.layer.2.DenseReluDense.wi.weight', 'decoder.block.9.layer.2.DenseReluDense.wo.weight', 'decoder.block.9.layer.2.layer_norm.weight', 'decoder.block.10.layer.0.SelfAttention.q.weight', 'decoder.block.10.layer.0.SelfAttention.k.weight', 'decoder.block.10.layer.0.SelfAttention.v.weight', 'decoder.block.10.layer.0.SelfAttention.o.weight', 'decoder.block.10.layer.0.layer_norm.weight', 'decoder.block.10.layer.1.EncDecAttention.q.weight', 'decoder.block.10.layer.1.EncDecAttention.k.weight', 'decoder.block.10.layer.1.EncDecAttention.v.weight', 'decoder.block.10.layer.1.EncDecAttention.o.weight', 'decoder.block.10.layer.1.layer_norm.weight', 'decoder.block.10.layer.2.DenseReluDense.wi.weight', 'decoder.block.10.layer.2.DenseReluDense.wo.weight', 'decoder.block.10.layer.2.layer_norm.weight', 'decoder.block.11.layer.0.SelfAttention.q.weight', 'decoder.block.11.layer.0.SelfAttention.k.weight', 'decoder.block.11.layer.0.SelfAttention.v.weight', 'decoder.block.11.layer.0.SelfAttention.o.weight', 'decoder.block.11.layer.0.layer_norm.weight', 'decoder.block.11.layer.1.EncDecAttention.q.weight', 'decoder.block.11.layer.1.EncDecAttention.k.weight', 'decoder.block.11.layer.1.EncDecAttention.v.weight', 'decoder.block.11.layer.1.EncDecAttention.o.weight', 'decoder.block.11.layer.1.layer_norm.weight', 'decoder.block.11.layer.2.DenseReluDense.wi.weight', 'decoder.block.11.layer.2.DenseReluDense.wo.weight', 'decoder.block.11.layer.2.layer_norm.weight', 'decoder.block.12.layer.0.SelfAttention.q.weight', 'decoder.block.12.layer.0.SelfAttention.k.weight', 'decoder.block.12.layer.0.SelfAttention.v.weight', 'decoder.block.12.layer.0.SelfAttention.o.weight', 'decoder.block.12.layer.0.layer_norm.weight', 'decoder.block.12.layer.1.EncDecAttention.q.weight', 'decoder.block.12.layer.1.EncDecAttention.k.weight', 'decoder.block.12.layer.1.EncDecAttention.v.weight', 'decoder.block.12.layer.1.EncDecAttention.o.weight', 'decoder.block.12.layer.1.layer_norm.weight', 'decoder.block.12.layer.2.DenseReluDense.wi.weight', 'decoder.block.12.layer.2.DenseReluDense.wo.weight', 'decoder.block.12.layer.2.layer_norm.weight', 'decoder.block.13.layer.0.SelfAttention.q.weight', 'decoder.block.13.layer.0.SelfAttention.k.weight', 'decoder.block.13.layer.0.SelfAttention.v.weight', 'decoder.block.13.layer.0.SelfAttention.o.weight', 'decoder.block.13.layer.0.layer_norm.weight', 'decoder.block.13.layer.1.EncDecAttention.q.weight', 'decoder.block.13.layer.1.EncDecAttention.k.weight', 'decoder.block.13.layer.1.EncDecAttention.v.weight', 'decoder.block.13.layer.1.EncDecAttention.o.weight', 'decoder.block.13.layer.1.layer_norm.weight', 'decoder.block.13.layer.2.DenseReluDense.wi.weight', 'decoder.block.13.layer.2.DenseReluDense.wo.weight', 'decoder.block.13.layer.2.layer_norm.weight', 'decoder.block.14.layer.0.SelfAttention.q.weight', 'decoder.block.14.layer.0.SelfAttention.k.weight', 'decoder.block.14.layer.0.SelfAttention.v.weight', 'decoder.block.14.layer.0.SelfAttention.o.weight', 'decoder.block.14.layer.0.layer_norm.weight', 'decoder.block.14.layer.1.EncDecAttention.q.weight', 'decoder.block.14.layer.1.EncDecAttention.k.weight', 'decoder.block.14.layer.1.EncDecAttention.v.weight', 'decoder.block.14.layer.1.EncDecAttention.o.weight', 'decoder.block.14.layer.1.layer_norm.weight', 'decoder.block.14.layer.2.DenseReluDense.wi.weight', 'decoder.block.14.layer.2.DenseReluDense.wo.weight', 'decoder.block.14.layer.2.layer_norm.weight', 'decoder.block.15.layer.0.SelfAttention.q.weight', 'decoder.block.15.layer.0.SelfAttention.k.weight', 'decoder.block.15.layer.0.SelfAttention.v.weight', 'decoder.block.15.layer.0.SelfAttention.o.weight', 'decoder.block.15.layer.0.layer_norm.weight', 'decoder.block.15.layer.1.EncDecAttention.q.weight', 'decoder.block.15.layer.1.EncDecAttention.k.weight', 'decoder.block.15.layer.1.EncDecAttention.v.weight', 'decoder.block.15.layer.1.EncDecAttention.o.weight', 'decoder.block.15.layer.1.layer_norm.weight', 'decoder.block.15.layer.2.DenseReluDense.wi.weight', 'decoder.block.15.layer.2.DenseReluDense.wo.weight', 'decoder.block.15.layer.2.layer_norm.weight', 'decoder.block.16.layer.0.SelfAttention.q.weight', 'decoder.block.16.layer.0.SelfAttention.k.weight', 'decoder.block.16.layer.0.SelfAttention.v.weight', 'decoder.block.16.layer.0.SelfAttention.o.weight', 'decoder.block.16.layer.0.layer_norm.weight', 'decoder.block.16.layer.1.EncDecAttention.q.weight', 'decoder.block.16.layer.1.EncDecAttention.k.weight', 'decoder.block.16.layer.1.EncDecAttention.v.weight', 'decoder.block.16.layer.1.EncDecAttention.o.weight', 'decoder.block.16.layer.1.layer_norm.weight', 'decoder.block.16.layer.2.DenseReluDense.wi.weight', 'decoder.block.16.layer.2.DenseReluDense.wo.weight', 'decoder.block.16.layer.2.layer_norm.weight', 'decoder.block.17.layer.0.SelfAttention.q.weight', 'decoder.block.17.layer.0.SelfAttention.k.weight', 'decoder.block.17.layer.0.SelfAttention.v.weight', 'decoder.block.17.layer.0.SelfAttention.o.weight', 'decoder.block.17.layer.0.layer_norm.weight', 'decoder.block.17.layer.1.EncDecAttention.q.weight', 'decoder.block.17.layer.1.EncDecAttention.k.weight', 'decoder.block.17.layer.1.EncDecAttention.v.weight', 'decoder.block.17.layer.1.EncDecAttention.o.weight', 'decoder.block.17.layer.1.layer_norm.weight', 'decoder.block.17.layer.2.DenseReluDense.wi.weight', 'decoder.block.17.layer.2.DenseReluDense.wo.weight', 'decoder.block.17.layer.2.layer_norm.weight', 'decoder.block.18.layer.0.SelfAttention.q.weight', 'decoder.block.18.layer.0.SelfAttention.k.weight', 'decoder.block.18.layer.0.SelfAttention.v.weight', 'decoder.block.18.layer.0.SelfAttention.o.weight', 'decoder.block.18.layer.0.layer_norm.weight', 'decoder.block.18.layer.1.EncDecAttention.q.weight', 'decoder.block.18.layer.1.EncDecAttention.k.weight', 'decoder.block.18.layer.1.EncDecAttention.v.weight', 'decoder.block.18.layer.1.EncDecAttention.o.weight', 'decoder.block.18.layer.1.layer_norm.weight', 'decoder.block.18.layer.2.DenseReluDense.wi.weight', 'decoder.block.18.layer.2.DenseReluDense.wo.weight', 'decoder.block.18.layer.2.layer_norm.weight', 'decoder.block.19.layer.0.SelfAttention.q.weight', 'decoder.block.19.layer.0.SelfAttention.k.weight', 'decoder.block.19.layer.0.SelfAttention.v.weight', 'decoder.block.19.layer.0.SelfAttention.o.weight', 'decoder.block.19.layer.0.layer_norm.weight', 'decoder.block.19.layer.1.EncDecAttention.q.weight', 'decoder.block.19.layer.1.EncDecAttention.k.weight', 'decoder.block.19.layer.1.EncDecAttention.v.weight', 'decoder.block.19.layer.1.EncDecAttention.o.weight', 'decoder.block.19.layer.1.layer_norm.weight', 'decoder.block.19.layer.2.DenseReluDense.wi.weight', 'decoder.block.19.layer.2.DenseReluDense.wo.weight', 'decoder.block.19.layer.2.layer_norm.weight', 'decoder.block.20.layer.0.SelfAttention.q.weight', 'decoder.block.20.layer.0.SelfAttention.k.weight', 'decoder.block.20.layer.0.SelfAttention.v.weight', 'decoder.block.20.layer.0.SelfAttention.o.weight', 'decoder.block.20.layer.0.layer_norm.weight', 'decoder.block.20.layer.1.EncDecAttention.q.weight', 'decoder.block.20.layer.1.EncDecAttention.k.weight', 'decoder.block.20.layer.1.EncDecAttention.v.weight', 'decoder.block.20.layer.1.EncDecAttention.o.weight', 'decoder.block.20.layer.1.layer_norm.weight', 'decoder.block.20.layer.2.DenseReluDense.wi.weight', 'decoder.block.20.layer.2.DenseReluDense.wo.weight', 'decoder.block.20.layer.2.layer_norm.weight', 'decoder.block.21.layer.0.SelfAttention.q.weight', 'decoder.block.21.layer.0.SelfAttention.k.weight', 'decoder.block.21.layer.0.SelfAttention.v.weight', 'decoder.block.21.layer.0.SelfAttention.o.weight', 'decoder.block.21.layer.0.layer_norm.weight', 'decoder.block.21.layer.1.EncDecAttention.q.weight', 'decoder.block.21.layer.1.EncDecAttention.k.weight', 'decoder.block.21.layer.1.EncDecAttention.v.weight', 'decoder.block.21.layer.1.EncDecAttention.o.weight', 'decoder.block.21.layer.1.layer_norm.weight', 'decoder.block.21.layer.2.DenseReluDense.wi.weight', 'decoder.block.21.layer.2.DenseReluDense.wo.weight', 'decoder.block.21.layer.2.layer_norm.weight', 'decoder.block.22.layer.0.SelfAttention.q.weight', 'decoder.block.22.layer.0.SelfAttention.k.weight', 'decoder.block.22.layer.0.SelfAttention.v.weight', 'decoder.block.22.layer.0.SelfAttention.o.weight', 'decoder.block.22.layer.0.layer_norm.weight', 'decoder.block.22.layer.1.EncDecAttention.q.weight', 'decoder.block.22.layer.1.EncDecAttention.k.weight', 'decoder.block.22.layer.1.EncDecAttention.v.weight', 'decoder.block.22.layer.1.EncDecAttention.o.weight', 'decoder.block.22.layer.1.layer_norm.weight', 'decoder.block.22.layer.2.DenseReluDense.wi.weight', 'decoder.block.22.layer.2.DenseReluDense.wo.weight', 'decoder.block.22.layer.2.layer_norm.weight', 'decoder.block.23.layer.0.SelfAttention.q.weight', 'decoder.block.23.layer.0.SelfAttention.k.weight', 'decoder.block.23.layer.0.SelfAttention.v.weight', 'decoder.block.23.layer.0.SelfAttention.o.weight', 'decoder.block.23.layer.0.layer_norm.weight', 'decoder.block.23.layer.1.EncDecAttention.q.weight', 'decoder.block.23.layer.1.EncDecAttention.k.weight', 'decoder.block.23.layer.1.EncDecAttention.v.weight', 'decoder.block.23.layer.1.EncDecAttention.o.weight', 'decoder.block.23.layer.1.layer_norm.weight', 'decoder.block.23.layer.2.DenseReluDense.wi.weight', 'decoder.block.23.layer.2.DenseReluDense.wo.weight', 'decoder.block.23.layer.2.layer_norm.weight']\n",
      "- This IS expected if you are initializing MT5ForConditionalGenerationWithLatentSpace from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MT5ForConditionalGenerationWithLatentSpace from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== parallelize ==================================================\n",
      "CPU times: user 40.8 s, sys: 14.1 s, total: 54.9 s\n",
      "Wall time: 38.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": "Embedding(129, 1024)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = MT5ForConditionalGenerationWithLatentSpace.from_pretrained(pretrained_dir, cache_dir=cache_dir, **latent_space_args)\n",
    "\n",
    "print('='*50, 'parallelize', '='*50)\n",
    "model.parallelize()\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda', index=0)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/home/hew/python/genhance/tensorboard/congen2/events.out.tfevents.1681060698.wenjia-genhance-spot-4'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/miniconda3/envs/genhance/lib/python3.8/site-packages/tensorboardX/record_writer.py:58\u001B[0m, in \u001B[0;36mopen_file\u001B[0;34m(path)\u001B[0m\n\u001B[1;32m     57\u001B[0m prefix \u001B[38;5;241m=\u001B[39m path\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m'\u001B[39m)[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m---> 58\u001B[0m factory \u001B[38;5;241m=\u001B[39m \u001B[43mREGISTERED_FACTORIES\u001B[49m\u001B[43m[\u001B[49m\u001B[43mprefix\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m factory\u001B[38;5;241m.\u001B[39mopen(path)\n",
      "\u001B[0;31mKeyError\u001B[0m: '/home/hew/python/genhance/tensorboard/congen2/events.out.tfevents.1681060698.wenjia-genhance-spot-4'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mPermissionError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[16], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m logging_dir \u001B[38;5;241m=\u001B[39m Path(logging_dir)\n\u001B[1;32m      2\u001B[0m logging_dir\u001B[38;5;241m.\u001B[39mmkdir(parents\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m----> 3\u001B[0m tb_writer \u001B[38;5;241m=\u001B[39m \u001B[43mSummaryWriter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogging_dir\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/genhance/lib/python3.8/site-packages/tensorboardX/writer.py:300\u001B[0m, in \u001B[0;36mSummaryWriter.__init__\u001B[0;34m(self, logdir, comment, purge_step, max_queue, flush_secs, filename_suffix, write_to_disk, log_dir, comet_config, **kwargs)\u001B[0m\n\u001B[1;32m    297\u001B[0m \u001B[38;5;66;03m# Initialize the file writers, but they can be cleared out on close\u001B[39;00m\n\u001B[1;32m    298\u001B[0m \u001B[38;5;66;03m# and recreated later as needed.\u001B[39;00m\n\u001B[1;32m    299\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfile_writer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mall_writers \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 300\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_file_writer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    302\u001B[0m \u001B[38;5;66;03m# Create default bins for histograms, see generate_testdata.py in tensorflow/tensorboard\u001B[39;00m\n\u001B[1;32m    303\u001B[0m v \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1E-12\u001B[39m\n",
      "File \u001B[0;32m~/miniconda3/envs/genhance/lib/python3.8/site-packages/tensorboardX/writer.py:348\u001B[0m, in \u001B[0;36mSummaryWriter._get_file_writer\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    345\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfile_writer\n\u001B[1;32m    347\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mall_writers \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfile_writer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 348\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfile_writer \u001B[38;5;241m=\u001B[39m \u001B[43mFileWriter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogdir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlogdir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    349\u001B[0m \u001B[43m                                  \u001B[49m\u001B[43mmax_queue\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_max_queue\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    350\u001B[0m \u001B[43m                                  \u001B[49m\u001B[43mflush_secs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_flush_secs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    351\u001B[0m \u001B[43m                                  \u001B[49m\u001B[43mfilename_suffix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_filename_suffix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    352\u001B[0m \u001B[43m                                  \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    353\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpurge_step \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    354\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfile_writer\u001B[38;5;241m.\u001B[39madd_event(\n\u001B[1;32m    355\u001B[0m             Event(step\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpurge_step, file_version\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbrain.Event:2\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "File \u001B[0;32m~/miniconda3/envs/genhance/lib/python3.8/site-packages/tensorboardX/writer.py:104\u001B[0m, in \u001B[0;36mFileWriter.__init__\u001B[0;34m(self, logdir, max_queue, flush_secs, filename_suffix)\u001B[0m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;66;03m# Sometimes PosixPath is passed in and we need to coerce it to\u001B[39;00m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;66;03m# a string in all cases\u001B[39;00m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;66;03m# TODO: See if we can remove this in the future if we are\u001B[39;00m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;66;03m# actually the ones passing in a PosixPath\u001B[39;00m\n\u001B[1;32m    103\u001B[0m logdir \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(logdir)\n\u001B[0;32m--> 104\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevent_writer \u001B[38;5;241m=\u001B[39m \u001B[43mEventFileWriter\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    105\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlogdir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_queue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflush_secs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilename_suffix\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcleanup\u001B[39m():\n\u001B[1;32m    108\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevent_writer\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/miniconda3/envs/genhance/lib/python3.8/site-packages/tensorboardX/event_file_writer.py:106\u001B[0m, in \u001B[0;36mEventFileWriter.__init__\u001B[0;34m(self, logdir, max_queue_size, flush_secs, filename_suffix)\u001B[0m\n\u001B[1;32m    104\u001B[0m directory_check(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_logdir)\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_event_queue \u001B[38;5;241m=\u001B[39m multiprocessing\u001B[38;5;241m.\u001B[39mQueue(max_queue_size)\n\u001B[0;32m--> 106\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ev_writer \u001B[38;5;241m=\u001B[39m \u001B[43mEventsWriter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    107\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_logdir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mevents\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilename_suffix\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    108\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flush_secs \u001B[38;5;241m=\u001B[39m flush_secs\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_closed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/genhance/lib/python3.8/site-packages/tensorboardX/event_file_writer.py:43\u001B[0m, in \u001B[0;36mEventsWriter.__init__\u001B[0;34m(self, file_prefix, filename_suffix)\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_file_name \u001B[38;5;241m=\u001B[39m file_prefix \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.out.tfevents.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(time\u001B[38;5;241m.\u001B[39mtime())[:\u001B[38;5;241m10\u001B[39m] \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m     41\u001B[0m     socket\u001B[38;5;241m.\u001B[39mgethostname() \u001B[38;5;241m+\u001B[39m filename_suffix\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outstanding_events \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m---> 43\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_py_recordio_writer \u001B[38;5;241m=\u001B[39m \u001B[43mRecordWriter\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_file_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;66;03m# Initialize an event instance.\u001B[39;00m\n\u001B[1;32m     45\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_event \u001B[38;5;241m=\u001B[39m event_pb2\u001B[38;5;241m.\u001B[39mEvent()\n",
      "File \u001B[0;32m~/miniconda3/envs/genhance/lib/python3.8/site-packages/tensorboardX/record_writer.py:179\u001B[0m, in \u001B[0;36mRecordWriter.__init__\u001B[0;34m(self, path)\u001B[0m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpath \u001B[38;5;241m=\u001B[39m path\n\u001B[1;32m    178\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_writer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 179\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_writer \u001B[38;5;241m=\u001B[39m \u001B[43mopen_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/genhance/lib/python3.8/site-packages/tensorboardX/record_writer.py:61\u001B[0m, in \u001B[0;36mopen_file\u001B[0;34m(path)\u001B[0m\n\u001B[1;32m     59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m factory\u001B[38;5;241m.\u001B[39mopen(path)\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n\u001B[0;32m---> 61\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mwb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mPermissionError\u001B[0m: [Errno 13] Permission denied: '/home/hew/python/genhance/tensorboard/congen2/events.out.tfevents.1681060698.wenjia-genhance-spot-4'"
     ]
    }
   ],
   "source": [
    "logging_dir = Path(logging_dir)\n",
    "logging_dir.mkdir(parents=True, exist_ok=True)\n",
    "tb_writer = SummaryWriter(logging_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hew/miniconda3/envs/genhance/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        'params'      : [p for n, p in model.named_parameters() if ('mi_head' not in n and not any(nd in n for nd in no_decay))],\n",
    "        'weight_decay': 0.01\n",
    "        },\n",
    "    {\n",
    "        'params'      : [p for n, p in model.named_parameters() if ('mi_head' not in n and any(nd in n for nd in no_decay))],\n",
    "        'weight_decay': 0.0\n",
    "        }\n",
    "    ]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "\n",
    "# lr scheduling\n",
    "lr_scheduler = get_scheduler(\n",
    "        'linear',\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps,\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len_train_loader:  3516\n",
      "num_train_epochs:  30\n",
      "n_iter:  105480\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "n_iter = int(num_train_epochs*len(train_loader))\n",
    "print(\"len_train_loader: \", len(train_loader))\n",
    "print(\"num_train_epochs: \", num_train_epochs)\n",
    "print(\"n_iter: \", n_iter)\n",
    "\n",
    "beta_t_list = frange_cycle_zero_linear(n_iter, start=0.0, stop=beta, n_cycle=10, ratio_increase=beta_ratio_increase,\n",
    "                                       ratio_zero=beta_ratio_zero)\n",
    "if beta_start_step > 0:\n",
    "    beta_t_list[:beta_start_step] = 0\n",
    "\n",
    "start_epoch = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_epoch: 1 current epoch: 1 global_step: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3516 [00:00<?, ?it/s]\u001B[A\n",
      "  0%|          | 1/3516 [00:02<2:02:52,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen_output.scores 85 torch.Size([32, 129]) torch.Size([32, 129])\n",
      "gen_logits torch.Size([32, 85, 129]) tensor([[-4.1270e+01, -1.4107e+01, -1.2900e+02,  ..., -1.2632e+02,\n",
      "         -1.3733e+00,  2.2185e-02],\n",
      "        [-1.5435e+02, -2.8106e+01, -1.9018e+02,  ..., -1.8501e+02,\n",
      "         -1.5003e+01,  2.3761e-02],\n",
      "        [-1.0998e+02, -2.6472e+01, -1.7997e+02,  ..., -1.7474e+02,\n",
      "         -6.0681e+00,  2.2423e-02],\n",
      "        ...,\n",
      "        [-6.5989e+01,  6.7634e+00, -1.3235e+02,  ..., -1.2796e+02,\n",
      "          4.1947e-01,  9.0156e-04],\n",
      "        [-7.4185e+01,  1.8686e+01, -1.4876e+02,  ..., -1.4401e+02,\n",
      "          2.2148e+00,  1.4316e-02],\n",
      "        [-9.5738e+01, -3.2716e+01, -1.6270e+02,  ..., -1.5683e+02,\n",
      "         -7.5621e+00, -2.2270e-02]], device='cuda:0')\n",
      "pc_gen_value_pred torch.Size([32, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[A\n",
      "  0%|          | 2/3516 [00:04<2:10:15,  2.22s/it]\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen_output.scores 85 torch.Size([32, 129]) torch.Size([32, 129])\n",
      "gen_logits torch.Size([32, 85, 129]) tensor([[-5.7531e+01, -2.4754e+01, -1.3775e+02,  ..., -1.3362e+02,\n",
      "         -7.9772e+00,  4.3027e-02],\n",
      "        [-1.1806e+02, -4.2069e+01, -1.7722e+02,  ..., -1.7296e+02,\n",
      "         -1.1798e+01,  1.1884e-02],\n",
      "        [-1.1035e+02, -2.6837e+01, -1.6398e+02,  ..., -1.5989e+02,\n",
      "         -1.6421e+01,  4.4236e-02],\n",
      "        ...,\n",
      "        [-1.0613e+02, -1.9419e+01, -1.8238e+02,  ..., -1.7763e+02,\n",
      "         -2.1746e+01,  1.0557e-02],\n",
      "        [-1.7954e+02, -2.1155e+01, -1.6548e+02,  ..., -1.6130e+02,\n",
      "         -1.7622e+01,  3.5217e-02],\n",
      "        [-1.6850e+02, -3.9683e+01, -1.8033e+02,  ..., -1.7795e+02,\n",
      "         -2.6483e+01,  6.3323e-02]], device='cuda:0')\n",
      "pc_gen_value_pred torch.Size([32, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/3516 [00:04<2:18:41,  2.37s/it]\n",
      "  0%|          | 0/30 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 74\u001B[0m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m pc_perturb_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstatic\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m     72\u001B[0m     pc_perturb \u001B[38;5;241m=\u001B[39m pc_perturb\n\u001B[0;32m---> 74\u001B[0m gen_output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dict_in_generate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_scores\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     75\u001B[0m \u001B[43m                            \u001B[49m\u001B[43mz_tar_edit_before_dec\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpc_perturb\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# change z_tar_edit_before_dec\u001B[39;00m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgen_output.scores\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;28mlen\u001B[39m(gen_output\u001B[38;5;241m.\u001B[39mscores), gen_output\u001B[38;5;241m.\u001B[39mscores[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mshape, gen_output\u001B[38;5;241m.\u001B[39mscores[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m     78\u001B[0m gen_logits \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mstack(gen_output\u001B[38;5;241m.\u001B[39mscores, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/genhance/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001B[0m, in \u001B[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m():\n\u001B[0;32m---> 27\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/python/genhance/transformers_custom/generation_utils.py:1023\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, **model_kwargs)\u001B[0m\n\u001B[1;32m   1018\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   1019\u001B[0m                 \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_return_sequences has to be 1, but is \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_return_sequences\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m when doing greedy search.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1020\u001B[0m                 )\n\u001B[1;32m   1022\u001B[0m     \u001B[38;5;66;03m# greedy search\u001B[39;00m\n\u001B[0;32m-> 1023\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgreedy_search\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1024\u001B[0m \u001B[43m            \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1025\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlogits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1026\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1027\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1028\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpad_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpad_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1029\u001B[0m \u001B[43m            \u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meos_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1030\u001B[0m \u001B[43m            \u001B[49m\u001B[43moutput_scores\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_scores\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1031\u001B[0m \u001B[43m            \u001B[49m\u001B[43mreturn_dict_in_generate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict_in_generate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1032\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1033\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1035\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m is_sample_gen_mode:\n\u001B[1;32m   1036\u001B[0m     \u001B[38;5;66;03m# get probability distribution warper\u001B[39;00m\n\u001B[1;32m   1037\u001B[0m     logits_warper \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_logits_warper(\n\u001B[1;32m   1038\u001B[0m             top_k\u001B[38;5;241m=\u001B[39mtop_k, top_p\u001B[38;5;241m=\u001B[39mtop_p, temperature\u001B[38;5;241m=\u001B[39mtemperature, num_beams\u001B[38;5;241m=\u001B[39mnum_beams\n\u001B[1;32m   1039\u001B[0m             )\n",
      "File \u001B[0;32m~/python/genhance/transformers_custom/generation_utils.py:1304\u001B[0m, in \u001B[0;36mGenerationMixin.greedy_search\u001B[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)\u001B[0m\n\u001B[1;32m   1301\u001B[0m model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_inputs_for_generation(input_ids, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs)\n\u001B[1;32m   1303\u001B[0m \u001B[38;5;66;03m# forward pass to get next token\u001B[39;00m\n\u001B[0;32m-> 1304\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1305\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1306\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1307\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1308\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1309\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1310\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(outputs) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m:\n\u001B[1;32m   1311\u001B[0m     next_token_logits \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mlogits[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, :]\n",
      "File \u001B[0;32m~/miniconda3/envs/genhance/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_slow_forward(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    888\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 889\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    890\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m itertools\u001B[38;5;241m.\u001B[39mchain(\n\u001B[1;32m    891\u001B[0m         _global_forward_hooks\u001B[38;5;241m.\u001B[39mvalues(),\n\u001B[1;32m    892\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks\u001B[38;5;241m.\u001B[39mvalues()):\n\u001B[1;32m    893\u001B[0m     hook_result \u001B[38;5;241m=\u001B[39m hook(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, result)\n",
      "File \u001B[0;32m~/python/genhance/transformers_custom/models/t5/modeling_t5.py:2201\u001B[0m, in \u001B[0;36mT5ForConditionalGenerationWithLatentSpace.forward\u001B[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, encoder_outputs, past_key_values, inputs_embeds, inputs_logits, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, contrast_targets, return_contrastive_head_pred, train_mi_head_step, output_perturbed_z_dec_logits, z_tar_edit_before_dec, z_tar_edit_noise_std_before_dec, return_only_value_pred, mask_similar_contrast_label, return_same_label_loss)\u001B[0m\n\u001B[1;32m   2198\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m decoder_attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2199\u001B[0m         decoder_attention_mask \u001B[38;5;241m=\u001B[39m decoder_attention_mask\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder\u001B[38;5;241m.\u001B[39mfirst_device)\n\u001B[0;32m-> 2201\u001B[0m decoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2202\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_input_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2203\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2204\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_inputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2205\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2206\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2207\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2208\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecoder_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2209\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_head_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2210\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2211\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2212\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2213\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2214\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2216\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m decoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   2217\u001B[0m \u001B[38;5;66;03m# print(\"sequence_output.shape: \", sequence_output.shape) # torch.Size([16, 84, 1024])\u001B[39;00m\n\u001B[1;32m   2218\u001B[0m \n\u001B[1;32m   2219\u001B[0m \u001B[38;5;66;03m# Set device for model parallelism\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/genhance/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_slow_forward(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    888\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 889\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    890\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m itertools\u001B[38;5;241m.\u001B[39mchain(\n\u001B[1;32m    891\u001B[0m         _global_forward_hooks\u001B[38;5;241m.\u001B[39mvalues(),\n\u001B[1;32m    892\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks\u001B[38;5;241m.\u001B[39mvalues()):\n\u001B[1;32m    893\u001B[0m     hook_result \u001B[38;5;241m=\u001B[39m hook(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, result)\n",
      "File \u001B[0;32m~/python/genhance/transformers_custom/models/t5/modeling_t5.py:1172\u001B[0m, in \u001B[0;36mT5Stack.forward\u001B[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_logits, inputs_embeds, head_mask, encoder_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001B[0m\n\u001B[1;32m   1169\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m==\u001B[39m v[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda:\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(k) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlast_device:\n\u001B[1;32m   1170\u001B[0m                 hidden_states \u001B[38;5;241m=\u001B[39m hidden_states\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda:\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(k \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m-> 1172\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfinal_layer_norm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1173\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(hidden_states)\n\u001B[1;32m   1175\u001B[0m \u001B[38;5;66;03m# Add last layer\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/genhance/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_slow_forward(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    888\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 889\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    890\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m itertools\u001B[38;5;241m.\u001B[39mchain(\n\u001B[1;32m    891\u001B[0m         _global_forward_hooks\u001B[38;5;241m.\u001B[39mvalues(),\n\u001B[1;32m    892\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks\u001B[38;5;241m.\u001B[39mvalues()):\n\u001B[1;32m    893\u001B[0m     hook_result \u001B[38;5;241m=\u001B[39m hook(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, result)\n",
      "File \u001B[0;32m~/python/genhance/transformers_custom/models/t5/modeling_t5.py:239\u001B[0m, in \u001B[0;36mT5LayerNorm.forward\u001B[0;34m(self, hidden_states)\u001B[0m\n\u001B[1;32m    236\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, hidden_states):\n\u001B[1;32m    237\u001B[0m     \u001B[38;5;66;03m# layer norm should always be calculated in float32\u001B[39;00m\n\u001B[1;32m    238\u001B[0m     variance \u001B[38;5;241m=\u001B[39m hidden_states\u001B[38;5;241m.\u001B[39mto(torch\u001B[38;5;241m.\u001B[39mfloat32)\u001B[38;5;241m.\u001B[39mpow(\u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mmean(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, keepdim\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m--> 239\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[43mhidden_states\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrsqrt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvariance\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvariance_epsilon\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    241\u001B[0m     \u001B[38;5;66;03m# convert into float16 if necessary\u001B[39;00m\n\u001B[1;32m    242\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m torch\u001B[38;5;241m.\u001B[39mfloat16:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in tqdm(range(start_epoch, num_train_epochs + 1)):\n",
    "    print('start_epoch:', start_epoch, 'current epoch:', epoch, 'global_step:', global_step)\n",
    "    for step, batch in enumerate(tqdm(train_loader)):\n",
    "        input_ids = batch['input_ids'].to(model.device)\n",
    "        labels = batch['labels'].to(model.device)\n",
    "        ddG_targets = batch['ddG'].to(model.device) if batch['ddG'] is not None else None\n",
    "        solubility_targets = batch['solubility'].to(model.device) if batch['solubility'] is not None else None\n",
    "        contrast_targets = [ddG_targets, solubility_targets]\n",
    "        model.zero_grad()\n",
    "\n",
    "        model_outputs = model(input_ids, labels=labels, contrast_targets=contrast_targets)\n",
    "        outputs, contrastive_loss, contrastive_value = model_outputs[0], model_outputs[1], model_outputs[2]\n",
    "        lm_loss = outputs.loss\n",
    "\n",
    "        '''reconstruction loss'''\n",
    "        total_loss = lm_loss\n",
    "\n",
    "        '''contrastive loss'''\n",
    "        if lambda_contrastive > 0:\n",
    "            if total_loss.device != contrastive_loss.device:\n",
    "                contrastive_loss = contrastive_loss.to(total_loss.device)\n",
    "            total_loss = total_loss + lambda_contrastive*contrastive_loss\n",
    "\n",
    "        if latent_space_type in ['vae', 'wae']:\n",
    "            z_regu_output = model_outputs[-1]\n",
    "            if type(z_regu_output) is dict:\n",
    "                z_regu_loss = z_regu_output['z_regu_loss']\n",
    "            else:\n",
    "                z_regu_loss = z_regu_output\n",
    "\n",
    "            if use_beta_schedule and global_step < len(beta_t_list):\n",
    "                '''no use here, use_beta_schedule == False'''\n",
    "                beta_z_regu = beta_t_list[global_step]\n",
    "            else:\n",
    "                if beta_start_step > 0 and global_step < beta_start_step:\n",
    "                    beta_z_regu = 0\n",
    "                else:\n",
    "                    beta_z_regu = beta  # constant value\n",
    "            if beta_z_regu > 0:\n",
    "                if total_loss.device != z_regu_loss.device:\n",
    "                    z_regu_loss = z_regu_loss.to(total_loss.device)\n",
    "\n",
    "                '''smoothness loss'''\n",
    "                total_loss = total_loss + beta_z_regu*z_regu_loss\n",
    "\n",
    "            if wae_z_enc_type != 'deterministic':\n",
    "                '''no use here, wae_z_enc_type == deterministic'''\n",
    "                z_logvar_L1 = z_regu_output['z_logvar_L1']\n",
    "                if lambda_logvar_L1 > 0 and beta_z_regu > 0:\n",
    "                    if total_loss.device != z_logvar_L1.device:\n",
    "                        z_logvar_L1 = z_logvar_L1.to(total_loss.device)\n",
    "\n",
    "                    # prevent z_logvar from being too large\n",
    "                    total_loss = total_loss + beta_z_regu*lambda_logvar_L1*z_logvar_L1\n",
    "\n",
    "                z_logvar_KL_penalty = z_regu_output['z_logvar_KL_penalty']\n",
    "                if lambda_logvar_KL > 0 and beta_z_regu > 0:\n",
    "                    if total_loss.device != z_logvar_KL_penalty.device:\n",
    "                        z_logvar_KL_penalty = z_logvar_KL_penalty.to(total_loss.device)\n",
    "\n",
    "                    # prevent z_logvar from diminishing\n",
    "                    total_loss = total_loss + beta_z_regu*lambda_logvar_KL*z_logvar_KL_penalty\n",
    "\n",
    "        # perturb cycle consistency loss\n",
    "        '''no use here, lambda_contrastive_perturb_cyc == 0.0'''\n",
    "        if lambda_contrastive_perturb_cyc > 0 and global_step > contrastive_perturb_cyc_start_step:\n",
    "            if pc_perturb_type == 'std':\n",
    "                contrastive_value_std = torch.std(contrastive_value)\n",
    "                pc_perturb = pc_perturb*contrastive_value_std.item()\n",
    "            elif pc_perturb_type == 'static':\n",
    "                pc_perturb = pc_perturb\n",
    "\n",
    "            gen_output = model.generate(input_ids, max_length=input_ids.shape[-1] + 1, return_dict_in_generate=True, output_scores=True,\n",
    "                                        z_tar_edit_before_dec=pc_perturb)  # change z_tar_edit_before_dec\n",
    "\n",
    "            # print('gen_output.scores', len(gen_output.scores), gen_output.scores[0].shape, gen_output.scores[1].shape)\n",
    "            gen_logits = torch.stack(gen_output.scores, dim=1)\n",
    "            # print('gen_logits', gen_logits.shape, gen_logits[0])\n",
    "            pc_gen_value_pred = model(inputs_logits=gen_logits, return_only_value_pred=True)\n",
    "            # print('pc_gen_value_pred', pc_gen_value_pred.shape)\n",
    "            if len(ddG_targets.shape) != 2:\n",
    "                ddG_targets = torch.unsqueeze(ddG_targets, dim=-1)\n",
    "            pc_gen_ddG_targets = ddG_targets + pc_perturb\n",
    "            pc_ddG_labels = torch.sign(ddG_targets - pc_gen_ddG_targets)*0.5 + 0.5\n",
    "            contrastive_preds = F.logsigmoid(contrastive_value - pc_gen_value_pred)\n",
    "            inverse_preds = F.logsigmoid(-1*(contrastive_value - pc_gen_value_pred))\n",
    "            if pc_ddG_labels.device != contrastive_preds.device:\n",
    "                pc_ddG_labels = pc_ddG_labels.to(contrastive_preds.device)\n",
    "            pc_losses = -pc_ddG_labels*contrastive_preds - (1 - pc_ddG_labels)*inverse_preds\n",
    "            contrastive_perturb_cyc_loss = pc_losses.mean()\n",
    "\n",
    "            if total_loss.device != contrastive_perturb_cyc_loss.device:\n",
    "                contrastive_perturb_cyc_loss = contrastive_perturb_cyc_loss.to(total_loss.device)\n",
    "            total_loss = total_loss + lambda_contrastive_perturb_cyc*contrastive_perturb_cyc_loss\n",
    "\n",
    "        if lambda_contrastive_cyc > 0 and global_step > contrastive_cyc_start_step:\n",
    "            model_outputs_2nd_forward = model(inputs_logits=outputs.logits, labels=labels, contrast_targets=contrast_targets)\n",
    "            contrastive_cyc_loss = model_outputs_2nd_forward[1]\n",
    "\n",
    "            if total_loss.device != contrastive_cyc_loss.device:\n",
    "                contrastive_cyc_loss = contrastive_cyc_loss.to(total_loss.device)\n",
    "\n",
    "            '''cycle consistency loss'''\n",
    "            total_loss = total_loss + lambda_contrastive_cyc*contrastive_cyc_loss\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        global_step += 1\n",
    "\n",
    "        if global_step%logging_steps == 0:\n",
    "            tb_writer.add_scalar(\"TRAIN/lr\", lr_scheduler.get_last_lr()[0], global_step)\n",
    "            tb_writer.add_scalar(\"TRAIN/contrastive_loss\", contrastive_loss, global_step)\n",
    "            tb_writer.add_scalar(\"TRAIN/lm_loss\", lm_loss, global_step)\n",
    "            if latent_space_type in ['vae', 'wae']:\n",
    "                tb_writer.add_scalar(\"TRAIN/z_regu_loss\", z_regu_loss, global_step)\n",
    "                tb_writer.add_scalar(\"TRAIN/beta_z_regu\", beta_z_regu, global_step)\n",
    "                if wae_z_enc_type != 'deterministic':\n",
    "                    tb_writer.add_scalar(\"TRAIN/z_logvar_L1\", z_logvar_L1, global_step)\n",
    "                    tb_writer.add_scalar(\"TRAIN/z_logvar_KL_penalty\", z_logvar_KL_penalty, global_step)\n",
    "\n",
    "            if lambda_contrastive_cyc > 0 and global_step > contrastive_cyc_start_step:\n",
    "                tb_writer.add_scalar(\"TRAIN/contrastive_cyc_loss\", contrastive_cyc_loss, global_step)\n",
    "\n",
    "            if lambda_contrastive_perturb_cyc > 0 and global_step > contrastive_perturb_cyc_start_step:\n",
    "                tb_writer.add_scalar(\"TRAIN/contrastive_perturb_cyc_loss\", contrastive_perturb_cyc_loss, global_step)\n",
    "\n",
    "        if global_step%eval_steps == 0:\n",
    "            model.eval()\n",
    "            eval_output = evaluate(model, eval_loader, do_mi=do_mi, latent_space_type=latent_space_type)\n",
    "            eval_lm_loss, eval_contrastive_loss, eval_ddG_spearman, eval_solubility_spearman = eval_output['lm_loss'], eval_output[\n",
    "                'contrastive_loss'], eval_output['ddG_spearman'], eval_output['solubility_spearman'],\n",
    "            tb_writer.add_scalar(\"EVAL/lm_loss\", eval_lm_loss, global_step)\n",
    "            tb_writer.add_scalar(\"EVAL/contrastive_loss\", eval_contrastive_loss, global_step)\n",
    "            tb_writer.add_scalar(\"EVAL/ddG_spearmanr\", eval_ddG_spearman, global_step)\n",
    "            tb_writer.add_scalar(\"EVAL/solubility_spearmanr\", eval_solubility_spearman, global_step)\n",
    "            if do_mi:\n",
    "                eval_mi_head_loss = eval_output['mi_head_loss']\n",
    "                tb_writer.add_scalar(\"EVAL/mi_head_loss\", eval_mi_head_loss, global_step)\n",
    "            if latent_space_type in ['vae', 'wae']:\n",
    "                eval_z_regu_loss = eval_output['z_regu_loss']\n",
    "                tb_writer.add_scalar(\"EVAL/z_regu_loss\", eval_z_regu_loss, global_step)\n",
    "            model.train()\n",
    "\n",
    "        if global_step%save_steps == 0:\n",
    "            print()\n",
    "            print('========== save step checkpoint ==========')\n",
    "            weights_name = \"pytorch_model.bin\"\n",
    "            ckpt_saved_dir = os.path.join(output_dir, f'step_{global_step}')\n",
    "            mkdir(ckpt_saved_dir)\n",
    "            saved_weights_file = os.path.join(ckpt_saved_dir, weights_name)\n",
    "            torch.save(model.state_dict(), saved_weights_file)\n",
    "            shutil.copy(src_json, ckpt_saved_dir)\n",
    "            # torch.save(args, ckpt_saved_dir + \"/training_bin\")\n",
    "            torch.save(optimizer.state_dict(), ckpt_saved_dir + \"/optimizer.pt\")\n",
    "            torch.save(lr_scheduler.state_dict(), ckpt_saved_dir + \"/scheduler.pt\")\n",
    "            ckpt_info = {'epoch': epoch, 'global_step': global_step}\n",
    "            torch.save(ckpt_info, ckpt_saved_dir + \"/ckpt_info.pt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
